{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post - Sim Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyplink import PyPlink\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import itertools\n",
    "import scipy.spatial.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import glob\n",
    "import random\n",
    "from random import shuffle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import msprime\n",
    "import allel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simuPOP\n",
    "\n",
    "import simuPOP.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rsq(geno_mat):\n",
    "    \"\"\"returns the squared pearson r for each pair of loci in a condensed distance matrix\"\"\"\n",
    "    return scipy.spatial.distance.pdist(geno_mat.T, lambda x, y: scipy.stats.pearsonr(x, y)[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_r2_fast(geno_mat):\n",
    "    norm_snps = (geno_mat - sp.mean(geno_mat, 0)) / sp.std(geno_mat, 0, ddof=1)\n",
    "    norm_snps = norm_snps.T\n",
    "    num_snps, num_indivs = norm_snps.shape\n",
    "    ld_mat = (sp.dot(norm_snps, norm_snps.T) / float(num_indivs-1))**2\n",
    "    return(ld_mat)\n",
    "\n",
    "def get_overall_mean(ld_mat):\n",
    "    return(ld_mat[np.triu_indices_from(ld_mat, k=1)].mean())\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\n",
    "    a = iter(iterable)\n",
    "    return itertools.izip(a, a)\n",
    "\n",
    "def get_nonoverlapping_mean(ld_mat):\n",
    "    # randomly selects non-overlapping pairs of loci and gets their mean\n",
    "    n_loci = ld_mat.shape[1]\n",
    "    indexes = range(n_loci)\n",
    "    random.shuffle(indexes)\n",
    "    a = indexes[0 : n_loci/2]\n",
    "    b = indexes[n_loci/2 : n_loci]\n",
    "    dat = ld_mat[a,b]\n",
    "    return (dat.mean())\n",
    "\n",
    "def shuffle_mat(mat):\n",
    "    # shuffles a square matrix by rows and columns, while keeping the association\n",
    "    shape1, shape2 = mat.shape \n",
    "    assert(shape1 == shape2)\n",
    "    new_order = range(shape1)\n",
    "    random.shuffle(new_order)\n",
    "    return(mat[new_order][:,new_order])\n",
    "\n",
    "def get_block_r2(ld_mat, nblocks):\n",
    "    # shuffles the matrix, then divides it into nblocks and takes the average of the meanr r2 within each block \n",
    "    ld_mat = shuffle_mat(ld_mat)\n",
    "    blocks = np.array_split(range(ld_mat.shape[1]), nblocks)\n",
    "    means = []\n",
    "    for block in blocks:\n",
    "        submat = ld_mat[block][:,block]\n",
    "        means.append(get_overall_mean(submat))\n",
    "        \n",
    "    sum_squares = np.sum(map(lambda x: x**2, means))\n",
    "    #    print submat\n",
    "    #print means \n",
    "    return(np.mean(means), sum_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weirFst(twopop_geno_mat, popsize):\n",
    "    ga = to_Genotype_array(twopop_geno_mat)\n",
    "    a, b, c = allel.weir_cockerham_fst(ga, subpops=[range(popsize), range(popsize, popsize*2)])\n",
    "    fst = np.sum(a) / (np.sum(a) + np.sum(b) + np.sum(c))\n",
    "    return fst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_Genotype_array(geno_mat):\n",
    "    a = np.where(geno_mat>0, 1, 0)\n",
    "    b = np.where(geno_mat>1, 1, 0)\n",
    "    return (allel.GenotypeArray(np.stack([a,b], axis =2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#allel.GenotypeArray(np.stack([a,b], axis =2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate temporal F given an ancestral and target pop\n",
    "def get_temporalF(ancestral, current):\n",
    "    \"\"\"\n",
    "    Nei and Tajima 1981 temporal F \n",
    "    \"\"\"\n",
    "    #make sure the loci in each pop line up!\n",
    "    P1 = ancestral.mean(0)/2.\n",
    "    P2 = current.mean(0)/2.\n",
    "    PP = (P1+P2)/2.0\n",
    "    TF_num = (P2-P1)**2\n",
    "    TF_denom = (PP - P1*P2)\n",
    "    TF_unweighted = (TF_num/TF_denom).mean() # mean of the ratios\n",
    "    TF_weighted = TF_num.sum()/TF_denom.sum() # ratio of the sums\n",
    "\n",
    "    return(TF_unweighted, TF_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Jorde_Ryman_F(ancestral, current):\n",
    "    xi = ancestral.mean(0)/2.\n",
    "    yi = current.mean(0)/2.\n",
    "    zi = (xi+yi)/2.\n",
    "    num = ((xi - yi)**2).sum()\n",
    "    denom = (zi*(1.0-zi)).sum()\n",
    "    return(num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Nei_Gst(pop1, pop2):\n",
    "    P1 = pop1.mean(0)/2.\n",
    "    P2 = pop2.mean(0)/2.\n",
    "    \n",
    "    sP1 = 1.0 - (P1**2 + (1-P1)**2)\n",
    "    sP2 = 1.0 - (P2**2 + (1-P2)**2)\n",
    "    Hexp = (sP1+sP2)/2.0\n",
    "    Pbar = (P1 + P2)/2.0\n",
    "    Htot = 1 - Pbar**2 - (1-Pbar)**2\n",
    "    F = 1.0 - Hexp/Htot\n",
    "    Fst_u = F.mean() # unweighted\n",
    "    \n",
    "    G_num = Htot - Hexp\n",
    "    G_denom = Htot\n",
    "    Fst_w = G_num.sum()/G_denom.sum() # weighted\n",
    "    #return(P1, P2)\n",
    "    #return(F)\n",
    "    #return(F, G)\n",
    "    return(Fst_u, Fst_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_pop1 = np.loadtxt('./share/feb25a/Ne-400_Chr-1/Ne-400_Chr-1_Frep-0.geno', dtype = 'int8', delimiter='\\t') \n",
    "#test_pop2 = np.loadtxt('./share/feb25a/Ne-400_Chr-1/Ne-400_Chr-1_Frep-1.geno', dtype = 'int8', delimiter='\\t')\n",
    "#test_ancestral_pop = np.loadtxt('./share/feb25a/Ne-400_Chr-1/Ne-400_Chr-1.inital.txt', dtype = 'int8', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_allele_counts(geno_array):\n",
    "    \"\"\"Used for calcualting Fst and the (2d) SFS\n",
    "    rows are loci, \n",
    "    columns are counts of specific alleles, sum of each row = sample size\n",
    "    \"\"\"\n",
    "    n_samples = geno_array.shape[0]\n",
    "    #print n_samples\n",
    "    derived_counts = geno_array.sum(axis = 0) # sum alleles over indiviudals \n",
    "    ancestral_counts = (n_samples*2) - derived_counts\n",
    "    #return derived_counts\n",
    "    return np.stack((ancestral_counts, derived_counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Hudson_Fst(pop1, pop2):\n",
    "    ac1 = get_allele_counts(pop1)\n",
    "    ac2 = get_allele_counts(pop2)\n",
    "    num, denom = allel.stats.hudson_fst(ac1, ac2)\n",
    "    fst_overall = np.sum(num)/np.sum(denom)\n",
    "    #print fst_overall\n",
    "    #return(num, denom)\n",
    "    return(fst_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_Patterson_f2(pop1, pop2):\n",
    "    \"\"\"numerator from hudsons Fst\"\"\"\n",
    "    ac1 = get_allele_counts(pop1)\n",
    "    ac2 = get_allele_counts(pop2)\n",
    "    return allel.stats.admixture.patterson_f2(ac1, ac2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_2dSFS(pop1, pop2):\n",
    "    ac1 = get_allele_counts(pop1)\n",
    "    ac2 = get_allele_counts(pop2)\n",
    "    return(allel.stats.sf.joint_sfs_folded(ac1, ac2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_analysis(target_dir,  L = [16, 64, 256], S = [25, 50, 100, 200], min_AC = 2, replicates = 1):\n",
    "    \"\"\"do_analysis should be pointed at a directory, \n",
    "    this will make it easier to manage the target replicate, ancestral pop, and pairs of populations\"\"\"\n",
    "    \n",
    "    results_file = os.path.join(target_dir, 'results.txt')\n",
    "    with open(results_file, 'w') as RESULTS:\n",
    "        #RESULTS.write('file\\tL\\tS\\tSrep\\tmean_r2\\tLocus_pairs\\n')\n",
    "        RESULTS.write('\\t' .join(['file','L','S','Srep', 'actual_pairs_r2', 'overall_r2', 'block_r2', 'block_sum_squares', 'nonoverlap_r2', \n",
    "                                  'temporalF_u', 'temporalF_w', 'Patterson_f2', 'Hudson_fst', 'Jorde_Ryman_F', 'weir_Fst', 'actual_nloci_temporal'\n",
    "                                 ]))\n",
    "        RESULTS.write('\\n')\n",
    "\n",
    "    ancestral_file = glob.glob(os.path.join(target_dir, '*.inital.txt'))\n",
    "    assert(len(ancestral_file)==1)\n",
    "    ancestral_file = ancestral_file[0]\n",
    "    ancestral_data = np.loadtxt(ancestral_file, dtype = 'int8', delimiter='\\t')\n",
    "    ancestral_nind = ancestral_data.shape[0]\n",
    "    \n",
    "    Frep_file_list = glob.glob(os.path.join(target_dir, '*.geno'))\n",
    "    print('found {} *.geno files'.format(len(Frep_file_list)))\n",
    "    cnt = 0\n",
    "    for infile in Frep_file_list:\n",
    "        cnt += 1\n",
    "        if cnt % 20 == 0:\n",
    "            print (\"working on file: {}\".format(infile))\n",
    "        base_data = np.loadtxt(infile, dtype = 'int8', delimiter='\\t')\n",
    "        rep_nind = base_data.shape[0]\n",
    "        for n_loci in L:\n",
    "            for n_sample in S:\n",
    "                for Srep in range(replicates):\n",
    "                    # pick individuals for the rep and ancestral \n",
    "                    # each rep file has up to 200 inds\n",
    "                    if n_sample <= rep_nind:\n",
    "                        pick_ind_rep = np.sort(np.random.choice(a = rep_nind, size = n_sample, replace = False))\n",
    "                        pick_ind_anc = np.sort(np.random.choice(a = ancestral_nind, size = n_sample, replace = False))\n",
    "\n",
    "                        subset_data = base_data[pick_ind_rep,:]\n",
    "                        # do the ancestral data matching the sample size, and as a awhole\n",
    "                        #ancestral_full = ancestral_data.copy()\n",
    "                        ancestral_subset = ancestral_data[pick_ind_anc,:]\n",
    "\n",
    "                        # filter for Allele count (AC) based on the sample\n",
    "                        rep_AC = subset_data.sum(0)\n",
    "                        both_AC = rep_AC + ancestral_subset.sum(0) # sum of AC in the acestral and base\n",
    "\n",
    "                        max_rep_AC = 2 * n_sample - min_AC\n",
    "                        max_both_AC = 4 * n_sample - min_AC\n",
    "\n",
    "                        lower_rep_test = min_AC <= rep_AC\n",
    "                        upper_rep_test = max_rep_AC >= rep_AC\n",
    "                        lower_both_test = min_AC <= both_AC\n",
    "                        upper_both_test = max_both_AC >= both_AC\n",
    "\n",
    "                        passed_AC_rep = np.logical_and(lower_rep_test, upper_rep_test)\n",
    "                        passed_AC_both = np.logical_and(lower_both_test, upper_both_test)\n",
    "\n",
    "                        # Pick the set of loci passing MAC filters\n",
    "                        # also subset the ancestral data in the same way!!\n",
    "                        r2_input = subset_data[:, passed_AC_rep]\n",
    "\n",
    "                        temporal_rep = subset_data[:, passed_AC_both]\n",
    "                        temporal_anc = ancestral_subset[:, passed_AC_both]\n",
    "                        #ancestral_full = ancestral_full[:,passed]\n",
    "\n",
    "                        remaining_loci_r2 = r2_input.shape[1]\n",
    "                        remaining_loci_temporal = temporal_rep.shape[1]\n",
    "\n",
    "                        if remaining_loci_r2 > n_loci:\n",
    "                            pick_loci_r2 = np.sort(np.random.choice(a = remaining_loci_r2, size = n_loci, replace = False))\n",
    "                            r2_input = r2_input[:, pick_loci_r2]\n",
    "\n",
    "                        actual_nloci_r2 = r2_input.shape[1]\n",
    "                        actual_pairs_r2 = (actual_nloci_r2 * (actual_nloci_r2-1))/2\n",
    "                        r2_mat = get_r2_fast(r2_input)\n",
    "\n",
    "                        block_r2, block_sum_squares = get_block_r2(r2_mat, 16)\n",
    "                        nonoverlap_r2 = get_nonoverlapping_mean(r2_mat)\n",
    "                        overall_r2 = get_overall_mean(r2_mat)\n",
    "\n",
    "                        if remaining_loci_temporal > n_loci:\n",
    "                            pick_loci_temporal = np.sort(np.random.choice(a = remaining_loci_temporal, size = n_loci, replace = False))\n",
    "                            temporal_rep = temporal_rep[:, pick_loci_temporal]\n",
    "                            temporal_anc = temporal_anc[:, pick_loci_temporal]\n",
    "\n",
    "                        actual_nloci_temporal = temporal_anc.shape[1]\n",
    "\n",
    "                        # actually do the temporal calculations\n",
    "                        temporalF_u, temporalF_w  = get_temporalF(temporal_anc, temporal_rep)\n",
    "                        Pf2 = get_Patterson_f2(temporal_anc, temporal_rep)\n",
    "                        Hfst = get_Hudson_Fst(temporal_anc, temporal_rep)\n",
    "                        Jorde_Ryman_F = get_Jorde_Ryman_F(temporal_anc, temporal_rep)\n",
    "                        forWeir = np.concatenate([temporal_anc, temporal_rep], axis =0).T\n",
    "                        weir_Fst = get_weirFst(forWeir, popsize = n_sample)\n",
    "\n",
    "                        #SFS = get_2dSFS(ancestral_full, subset_data)\n",
    "                        #SFS = SFS.flatten()\n",
    "\n",
    "                        with open(results_file, 'a') as RESULTS:\n",
    "                            RESULTS.write('{}\\t'.format(infile))\n",
    "                            for xx in [n_loci, n_sample, Srep, actual_pairs_r2, overall_r2, block_r2, block_sum_squares, nonoverlap_r2]:\n",
    "                                RESULTS.write('{}\\t'.format(xx))\n",
    "                            for xx in [temporalF_u, temporalF_w, Pf2, Hfst, Jorde_Ryman_F, weir_Fst]:\n",
    "                                RESULTS.write('{}\\t'.format(xx))\n",
    "                            for xx in [actual_nloci_temporal]:\n",
    "                                RESULTS.write('{}'.format(xx))\n",
    "                            #for xx in [SFS]:\n",
    "                            #    RESULTS.write(','.join([str(x) for x in SFS]))\n",
    "                            #    RESULTS.write('\\t')                            \n",
    "                            RESULTS.write('\\n')\n",
    "            \n",
    "    # pairwise analysis of populations\n",
    "    pair_results_file = os.path.join(target_dir, 'results.pairs.txt')\n",
    "    with open(pair_results_file, 'w') as PAIR_RESULTS:\n",
    "        PAIR_RESULTS.write('\\t' .join(['file1','file2', 'S', 'L', 'Srep',\n",
    "                             'Patterson_f2', 'Hudson_fst', 'weir_Fst', 'NeiGst_unweghted', 'NeiGst_weghted', 'actual_loci'\n",
    "                             ]))\n",
    "        PAIR_RESULTS.write('\\n')\n",
    "\n",
    "    for file1, file2 in zip(Frep_file_list,Frep_file_list[1:])[::2]:\n",
    "        pop1_data = np.loadtxt(file1, dtype = 'int8', delimiter='\\t')\n",
    "        pop2_data = np.loadtxt(file2, dtype = 'int8', delimiter='\\t')\n",
    "        pop1_nind = pop1_data.shape[0]\n",
    "        pop2_nind = pop2_data.shape[0]\n",
    "        #print file1, file2\n",
    "        for n_loci in L:\n",
    "            for n_sample in S:\n",
    "                for Srep in range(replicates):\n",
    "                    if n_sample <= rep_nind:\n",
    "                        pick_inds = np.sort(np.random.choice(a = pop1_nind, size = n_sample, replace = False))\n",
    "                        subset_pop1 = pop1_data[pick_inds, :]\n",
    "                        pick_inds = np.sort(np.random.choice(a = pop2_nind, size = n_sample, replace = False))\n",
    "                        subset_pop2 = pop2_data[pick_inds, :]\n",
    "\n",
    "                        # filter for combined MAC\n",
    "                        both_AC = subset_pop1.sum(0) + subset_pop2.sum(0)\n",
    "                        max_both_AC = 4 * n_sample - min_AC\n",
    "                        lower_both_test = min_AC <= both_AC\n",
    "                        upper_both_test = max_both_AC >= both_AC\n",
    "                        passed_AC_both = np.logical_and(lower_both_test, upper_both_test)\n",
    "\n",
    "                        subset_pop1 = subset_pop1[:, passed_AC_both]\n",
    "                        subset_pop2 = subset_pop2[:, passed_AC_both]\n",
    "\n",
    "                        # random subset of loci\n",
    "                        remaining_loci = subset_pop1.shape[1]\n",
    "                        if remaining_loci > n_loci:      \n",
    "                            pick_loci = np.sort(np.random.choice(a = remaining_loci, size = n_loci, replace = False))\n",
    "                            subset_pop1 = subset_pop1[:,pick_loci]\n",
    "                            subset_pop2 = subset_pop2[:,pick_loci]\n",
    "                        actual_loci = subset_pop1.shape[1]\n",
    "\n",
    "                        # statistics\n",
    "                        Pf2 = get_Patterson_f2(subset_pop1, subset_pop2)\n",
    "                        Hfst = get_Hudson_Fst(subset_pop1, subset_pop2)\n",
    "                        \n",
    "                        forWeir = np.concatenate([subset_pop1, subset_pop2], axis =0).T\n",
    "                        weir_Fst = get_weirFst(forWeir, popsize = n_sample)\n",
    "                        \n",
    "                        NeiGst_unweighted, NeiGst_weighted = get_Nei_Gst(subset_pop1, subset_pop2)\n",
    "\n",
    "\n",
    "                        # TODO add Hudson Fst back in\n",
    "                        with open(pair_results_file, 'a') as PAIR_RESULTS:\n",
    "                            for xx in [file1, file2, n_sample, n_loci, Srep,\n",
    "                                      Pf2, Hfst, weir_Fst, NeiGst_unweighted, NeiGst_weighted]:\n",
    "                                PAIR_RESULTS.write('{}\\t'.format(xx))\n",
    "                            for xx in [actual_loci]:\n",
    "                                PAIR_RESULTS.write('{}'.format(xx))\n",
    "                            PAIR_RESULTS.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_genotype_ld(sim_data):\n",
    "    first_r2_mat = allel.stats.ld.rogers_huff_r_between(get_diploid_genotypes(sim_data), get_diploid_genotypes(sim_data), fill=np.nan)\n",
    "    plt.imshow(first_r2_mat, interpolation=\"none\", vmin=0, vmax=1, cmap=\"Blues\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not used !!!\n",
    "def wrap_r2(file_list, output_file, MAF, L = [16, 64, 256, 1024, 4096], S = [50, 100], replicates = 1):\n",
    "    with open(output_file, 'w') as OUTFILE:\n",
    "        OUTFILE.write('file\\tL\\tS\\tSrep\\tmean_r2\\tLocus_pairs\\n') # write header\n",
    "    for infile in file_list:\n",
    "        print \"working on file: {}\".format(infile)\n",
    "        base_data = np.loadtxt(infile, dtype = 'int8', delimiter='\\t')\n",
    "        for n_loci in L:\n",
    "            for n_sample in S:\n",
    "                for Srep in range(replicates):\n",
    "                    # sample inds\n",
    "                    pick_inds = np.sort(np.random.choice(a = 200, size = n_sample, replace = False))\n",
    "                    subset_data = base_data[pick_inds,:]\n",
    "                    # filter for MAF based on the freqs in the sample\n",
    "                    subset_data = filter_MAF(subset_data, MAF = MAF)\n",
    "                    # select n_loci from among those loci passing MAF filters\n",
    "                    remaining_loci = subset_data.shape[1]\n",
    "                    pick_loci = np.sort(np.random.choice(a = remaining_loci, size = n_loci, replace = False))\n",
    "                    subset_data = subset_data[:,pick_loci]\n",
    "                    actual_loci = subset_data.shape[1]\n",
    "                    n_locus_pairs = (actual_loci * (actual_loci-1))/2\n",
    "                    #print pick_inds, pick_loci\n",
    "                    #return(subset_data)\n",
    "                    mean_rsq = get_r2_fast(subset_data).mean()\n",
    "                    with open(output_file, 'a') as OUTFILE:\n",
    "                        OUTFILE.write('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(infile, n_loci, n_sample, Srep, mean_rsq, n_locus_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Robin's code\n",
    "GetFst <- function(yy)     {\n",
    "OldP = yy[1,]\n",
    "P2 = yy[2,]\n",
    "Fst = seq(1:length(L))\n",
    "SumP1 = 1 - (P2^2 + (1-P2)^2)\n",
    "SumP2 = 1 - (OldP^2 + (1-OldP)^2)\n",
    "Hexp = (SumP1+SumP2)/2\n",
    "Pbar = (P2 + OldP)/2\n",
    "Htot = 1 - Pbar^2 - (1-Pbar)^2 \n",
    "F = 1 - Hexp/Htot\n",
    "Fst[1] = mean(F,na.rm=TRUE)\n",
    "  for (k in 2:length(L))  {\n",
    "    FF = F[1:L[k]]\n",
    "    Fst[k] = mean(FF)  }  # end for k                            \n",
    "return(Fst)  } # end function\n",
    "\n",
    "is.odd <- function(x) x %% 2 != 0"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
